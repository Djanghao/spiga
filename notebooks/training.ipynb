{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from spiga.models.spiga import SPIGA\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load image and bbox\n",
    "image = cv2.imread(\"../assets/colab/image_sportsfan.jpg\")\n",
    "with open('../assets/colab/bbox_sportsfan.json') as jsonfile:\n",
    "    bbox = json.load(jsonfile)['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spiga.models.spiga import SPIGA\n",
    "\n",
    "spiga = SPIGA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pkg_resources\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "weights_path_dft = pkg_resources.resource_filename('spiga', 'models/weights')\n",
    "\n",
    "import spiga.inference.pretreatment as pretreat\n",
    "from spiga.models.spiga import SPIGA\n",
    "from spiga.inference.config import ModelConfig\n",
    "\n",
    "\n",
    "class SPIGAFramework:\n",
    "\n",
    "    def __init__(self, model_cfg: ModelConfig(), gpus=[0], load3DM=True):\n",
    "\n",
    "        # Parameters\n",
    "        self.model_cfg = model_cfg\n",
    "        self.gpus = gpus\n",
    "\n",
    "        # Pretreatment initialization\n",
    "        self.transforms = pretreat.get_transformers(self.model_cfg)\n",
    "\n",
    "        # SPIGA model\n",
    "        self.model_inputs = ['image', \"model3d\", \"cam_matrix\"]\n",
    "        self.model = SPIGA(num_landmarks=model_cfg.dataset.num_landmarks,\n",
    "                           num_edges=model_cfg.dataset.num_edges)\n",
    "\n",
    "        # Load weights and set model\n",
    "        weights_path = self.model_cfg.model_weights_path\n",
    "        if weights_path is None:\n",
    "            weights_path = weights_path_dft\n",
    "\n",
    "        # if self.model_cfg.load_model_url:\n",
    "        #     model_state_dict = torch.hub.load_state_dict_from_url(self.model_cfg.model_weights_url,\n",
    "        #                                                           model_dir=weights_path,\n",
    "        #                                                           file_name=self.model_cfg.model_weights)\n",
    "        # else:\n",
    "        #     weights_file = os.path.join(weights_path, self.model_cfg.model_weights)\n",
    "        #     model_state_dict = torch.load(weights_file)\n",
    "\n",
    "        # self.model.load_state_dict(model_state_dict)\n",
    "        # self.model = self.model.cuda(gpus[0])\n",
    "        self.model.eval()\n",
    "        print('SPIGA model loaded!')\n",
    "\n",
    "        # Load 3D model and camera intrinsic matrix\n",
    "        if load3DM:\n",
    "            loader_3DM = pretreat.AddModel3D(model_cfg.dataset.ldm_ids,\n",
    "                                             ftmap_size=model_cfg.ftmap_size,\n",
    "                                             focal_ratio=model_cfg.focal_ratio,\n",
    "                                             totensor=True)\n",
    "            params_3DM = self._data2device(loader_3DM())\n",
    "            self.model3d = params_3DM['model3d']\n",
    "            self.cam_matrix = params_3DM['cam_matrix']\n",
    "\n",
    "    def inference(self, image, bboxes):\n",
    "        batch_crops, crop_bboxes = self.pretreat(image, bboxes)\n",
    "        outputs = self.net_forward(batch_crops)\n",
    "        features = self.postreatment(outputs, crop_bboxes, bboxes)\n",
    "        return features\n",
    "\n",
    "    def pretreat(self, image, bboxes):\n",
    "        crop_bboxes = []\n",
    "        crop_images = []\n",
    "        for bbox in bboxes:\n",
    "            sample = {'image': copy.deepcopy(image),\n",
    "                      'bbox': copy.deepcopy(bbox)}\n",
    "            sample_crop = self.transforms(sample)\n",
    "            crop_bboxes.append(sample_crop['bbox'])\n",
    "            crop_images.append(sample_crop['image'])\n",
    "\n",
    "        # Images to tensor and device\n",
    "        batch_images = torch.tensor(np.array(crop_images), dtype=torch.float)\n",
    "        # batch_images = self._data2device(batch_images)\n",
    "        # Batch 3D model and camera intrinsic matrix\n",
    "        batch_model3D = self.model3d.unsqueeze(0).repeat(len(bboxes), 1, 1)\n",
    "        batch_cam_matrix = self.cam_matrix.unsqueeze(0).repeat(len(bboxes), 1, 1)\n",
    "\n",
    "        # SPIGA inputs\n",
    "        model_inputs = [batch_images, batch_model3D, batch_cam_matrix]\n",
    "        return model_inputs, crop_bboxes\n",
    "\n",
    "    def net_forward(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "\n",
    "    def postreatment(self, output, crop_bboxes, bboxes):\n",
    "        features = {}\n",
    "        crop_bboxes = np.array(crop_bboxes)\n",
    "        bboxes = np.array(bboxes)\n",
    "\n",
    "        if 'Landmarks' in output.keys():\n",
    "            landmarks = output['Landmarks'][-1].cpu().detach().numpy()\n",
    "            landmarks = landmarks.transpose((1, 0, 2))\n",
    "            landmarks = landmarks*self.model_cfg.image_size\n",
    "            landmarks_norm = (landmarks - crop_bboxes[:, 0:2]) / crop_bboxes[:, 2:4]\n",
    "            landmarks_out = (landmarks_norm * bboxes[:, 2:4]) + bboxes[:, 0:2]\n",
    "            landmarks_out = landmarks_out.transpose((1, 0, 2))\n",
    "            features['landmarks'] = landmarks_out.tolist()\n",
    "\n",
    "        # Pose output\n",
    "        if 'Pose' in output.keys():\n",
    "            pose = output['Pose'].cpu().detach().numpy()\n",
    "            features['headpose'] = pose.tolist()\n",
    "\n",
    "        return features\n",
    "\n",
    "    def select_inputs(self, batch):\n",
    "        inputs = []\n",
    "        for ft_name in self.model_inputs:\n",
    "            data = batch[ft_name]\n",
    "            inputs.append(self._data2device(data.type(torch.float)))\n",
    "        return inputs\n",
    "\n",
    "    def _data2device(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data_var = data\n",
    "            for data_id, v_data in enumerate(data):\n",
    "                data_var[data_id] = self._data2device(v_data)\n",
    "        if isinstance(data, dict):\n",
    "            data_var = data\n",
    "            for k, v in data.items():\n",
    "                data[k] = self._data2device(v)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                data_var = data.cuda(device=self.gpus[0], non_blocking=True)\n",
    "        return data_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPIGA model loaded!\n"
     ]
    }
   ],
   "source": [
    "from spiga.inference.config import ModelConfig\n",
    "\n",
    "dataset = 'wflw'\n",
    "processor = SPIGAFramework(ModelConfig(dataset))\n",
    "# features = processor.inference(image, [bbox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs, crop_bboxes = processor.pretreat(image, [bbox])\n",
    "[batch_images, batch_model3D, batch_cam_matrix] = model_inputs\n",
    "\n",
    "# batch_images = batch_images.expand(4, -1, -1, -1)\n",
    "# batch_model3D = batch_model3D.expand(4, -1, -1, -1)\n",
    "# batch_cam_matrix = batch_cam_matrix.expand(4, -1, -1, -1)\n",
    "# model_inputs = [batch_images, batch_model3D, batch_cam_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_images.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 98, 3])\n",
      "torch.Size([1, 3, 3])\n",
      "[ 48.          50.2230171  160.         155.55396581]\n"
     ]
    }
   ],
   "source": [
    "print(batch_images.shape)\n",
    "print(batch_model3D.shape)\n",
    "print(batch_cam_matrix.shape)\n",
    "print(crop_bboxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([4, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "spiga = SPIGA(num_landmarks=98, num_edges=15)\n",
    "# pts_proj, features = model.backbone_forward(model_inputs)\n",
    "imgs = model_inputs[0]\n",
    "model3d = model_inputs[1]\n",
    "cam_matrix = model_inputs[2]\n",
    "\n",
    "print(imgs.shape)\n",
    "# duplicate it to make batch data\n",
    "imgs = imgs.expand(4,-1,-1,-1)\n",
    "print(imgs.shape)\n",
    "\n",
    "# HourGlass Forward\n",
    "features = spiga.visual_cnn(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 64, 64])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['VisualField'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spiga.models.gnn.pose_proj as pproj\n",
    "\n",
    "pose_raw = features['HGcore'][-1]\n",
    "B, L, _, _ = pose_raw.shape\n",
    "pose = pose_raw.reshape(B, L)\n",
    "pose = spiga.pose_fc(pose)\n",
    "features['Pose'] = pose.clone()\n",
    "\n",
    "euler = pose[:, 0:3]\n",
    "trl = pose[:, 3:]\n",
    "rot = pproj.euler_to_rotation_matrix(euler)\n",
    "\n",
    "model3d = model3d.to('cpu')\n",
    "cam_matrix = cam_matrix.to('cpu')\n",
    "\n",
    "pts_proj = pproj.projectPoints(model3d, rot, trl, cam_matrix)\n",
    "pts_proj = pts_proj / spiga.visual_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98, 2])\n",
      "dict_keys(['VisualField', 'HGcore', 'Pose'])\n"
     ]
    }
   ],
   "source": [
    "print(pts_proj[0].shape)\n",
    "print(features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "visual_field = features['VisualField'][-1]\n",
    "print(visual_field.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/anaconda3/envs/python3.9/lib/python3.9/site-packages/torch/nn/functional.py:4377: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/hao/anaconda3/envs/python3.9/lib/python3.9/site-packages/torch/nn/functional.py:4316: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedded_ft = spiga.extract_embedded(pts_proj, visual_field, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 98, 512])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "B, L, _ = pts_proj.shape  # Pts_proj range:[0,1]\n",
    "centers = pts_proj + 0.5 / spiga.visual_res  # BxLx2\n",
    "centers = centers.reshape(B * L, 2)  # B*Lx2\n",
    "theta_trl = (-1 + centers * 2).unsqueeze(-1)  # BxLx2x1\n",
    "theta_s = spiga.theta_S[step]  # 2x2\n",
    "theta_s = theta_s.repeat(B * L, 1, 1)  # B*Lx2x2\n",
    "theta = torch.cat((theta_s, theta_trl), -1)  # B*Lx2x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([392, 256, 7, 7])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate crop grid\n",
    "B, C, _, _ = visual_field.shape\n",
    "grid = torch.nn.functional.affine_grid(theta, (B * L, C, spiga.kwindow, spiga.kwindow))\n",
    "grid = grid.reshape(B, L, spiga.kwindow, spiga.kwindow, 2)\n",
    "grid = grid.reshape(B, L, spiga.kwindow * spiga.kwindow, 2)\n",
    "\n",
    "# Crop windows\n",
    "crops = torch.nn.functional.grid_sample(visual_field, grid, padding_mode=\"border\")  # BxCxLxK*K\n",
    "crops = crops.transpose(1, 2)  # BxLxCxK*K\n",
    "crop_tmp = crops.clone()\n",
    "crops = crops.reshape(B * L, C, spiga.kwindow, spiga.kwindow)\n",
    "\n",
    "crops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 98, 256, 7, 7])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_tmp = crop_tmp.reshape(B, L, C, spiga.kwindow, spiga.kwindow)\n",
    "crop_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 64, 64])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_field.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.5762, 3.5762, 3.5762,  ..., 3.5762, 3.5762, 3.5762],\n",
       "         [3.5762, 3.5762, 3.5762,  ..., 3.5762, 3.5762, 3.5762],\n",
       "         [3.5762, 3.5762, 3.5762,  ..., 3.5762, 3.5762, 3.5762],\n",
       "         ...,\n",
       "         [3.5762, 3.5762, 3.5762,  ..., 3.5762, 3.5762, 3.5762],\n",
       "         [3.5762, 3.5762, 3.5762,  ..., 3.5762, 3.5762, 3.5762],\n",
       "         [3.5762, 3.5762, 3.5762,  ..., 3.5762, 3.5762, 3.5762]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.8879, 0.8879, 0.8879,  ..., 0.8879, 0.8879, 0.8879],\n",
       "         [0.8879, 0.8879, 0.8879,  ..., 0.8879, 0.8879, 0.8879],\n",
       "         [0.8879, 0.8879, 0.8879,  ..., 0.8879, 0.8879, 0.8879],\n",
       "         ...,\n",
       "         [0.8879, 0.8879, 0.8879,  ..., 0.8879, 0.8879, 0.8879],\n",
       "         [0.8879, 0.8879, 0.8879,  ..., 0.8879, 0.8879, 0.8879],\n",
       "         [0.8879, 0.8879, 0.8879,  ..., 0.8879, 0.8879, 0.8879]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.1124, 1.1124, 1.1124,  ..., 1.1124, 1.1124, 1.1124],\n",
       "         [1.1124, 1.1124, 1.1124,  ..., 1.1124, 1.1124, 1.1124],\n",
       "         [1.1124, 1.1124, 1.1124,  ..., 1.1124, 1.1124, 1.1124],\n",
       "         ...,\n",
       "         [1.1124, 1.1124, 1.1124,  ..., 1.1124, 1.1124, 1.1124],\n",
       "         [1.1124, 1.1124, 1.1124,  ..., 1.1124, 1.1124, 1.1124],\n",
       "         [1.1124, 1.1124, 1.1124,  ..., 1.1124, 1.1124, 1.1124]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2373, 0.2373, 0.2373,  ..., 0.2373, 0.2373, 0.2373],\n",
       "         [0.2373, 0.2373, 0.2373,  ..., 0.2373, 0.2373, 0.2373],\n",
       "         [0.2373, 0.2373, 0.2373,  ..., 0.2373, 0.2373, 0.2373],\n",
       "         ...,\n",
       "         [0.2373, 0.2373, 0.2373,  ..., 0.2373, 0.2373, 0.2373],\n",
       "         [0.2373, 0.2373, 0.2373,  ..., 0.2373, 0.2373, 0.2373],\n",
       "         [0.2373, 0.2373, 0.2373,  ..., 0.2373, 0.2373, 0.2373]]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_tmp[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([392, 512, 1, 1])\n",
      "torch.Size([4, 98, 512])\n"
     ]
    }
   ],
   "source": [
    "# Flatten features\n",
    "visual_ft = spiga.conv_window[step](crops)\n",
    "print(visual_ft.shape)\n",
    "_, Cout, _, _ = visual_ft.shape\n",
    "visual_ft = visual_ft.reshape(B, L, Cout)\n",
    "print(visual_ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/anaconda3/envs/python3.9/lib/python3.9/site-packages/torch/nn/functional.py:4377: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/hao/anaconda3/envs/python3.9/lib/python3.9/site-packages/torch/nn/functional.py:4316: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Params compute only once\n",
    "gat_prob = []\n",
    "features['Landmarks'] = []\n",
    "for step in range(spiga.steps):\n",
    "    # Features generation\n",
    "    embedded_ft = spiga.extract_embedded(pts_proj, visual_field, step)\n",
    "\n",
    "    # GAT inference\n",
    "    offset, gat_prob = spiga.gcn[step](embedded_ft, gat_prob)\n",
    "    offset = F.hardtanh(offset)\n",
    "\n",
    "    # Update coordinates\n",
    "    pts_proj = pts_proj + spiga.offset_ratio[step] * offset\n",
    "    features['Landmarks'].append(pts_proj.clone())\n",
    "    break\n",
    "\n",
    "features['GATProb'] = gat_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['VisualField', 'HGcore', 'Pose', 'Landmarks', 'GATProb'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 98, 2])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(features.keys())\n",
    "features['Landmarks'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 98, 512])\n"
     ]
    }
   ],
   "source": [
    "shape_ft = spiga.calculate_distances(pts_proj)\n",
    "shape_ft = spiga.shape_encoder[step](shape_ft)\n",
    "# Addition\n",
    "embedded_ft = visual_ft + shape_ft\n",
    "print(embedded_ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 98, 98])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat_prob[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT inference\n",
    "offset, gat_prob = spiga.gcn[step](embedded_ft, gat_prob)\n",
    "offset = F.hardtanh(offset)\n",
    "\n",
    "# Update coordinates\n",
    "pts_proj = pts_proj + spiga.offset_ratio[step] * offset\n",
    "features['Landmarks'].append(pts_proj.clone())\n",
    "\n",
    "features['GATProb'] = gat_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2.0.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
